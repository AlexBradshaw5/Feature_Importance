{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d840fceb",
   "metadata": {},
   "source": [
    "<h1><center>Feature Importance</center></h1>\n",
    "\n",
    "<h2><center>By Alex Bradshaw</center></h2>\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "   Feature importance is how important our features are (duh), but what does that really mean and why does it matter? Specifically, it means what features actually have significant power predicting our target variable. If a red ball bounced more than a blue ball, would we conclude red balls generally bounce higher than blue ones? Maybe for this example but it doesn't generalize well, and it's more important to generalize well than to be correct for any specific situation. In the real world, we would add as many features as possible at first into a model, and then select which ones generalize the best for unknown data. \n",
    "\n",
    "## 2. Adding \"Noise\"\n",
    "My personal favorite feature importance method is by adding random noise. This technique is usually best used in random forest models since its feature importance attribute isn't inherently meaniningful. The idea behind this method is that the noise feature will get assigned some feature importance value. Since it's literally random mumbo jumbo, we know that it should not predict very well at all. Therefore anything given an importance value less than it we should exclude. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e80b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbdef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forest_run(X, y, rf=RandomForestRegressor(random_state=13)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)\n",
    "    X_train['noise'] = np.random.choice(np.arange(10), size = len(X_train))\n",
    "    rf.fit(X_train, y_train)\n",
    "    arr = [(rf.feature_names_in_[x], rf.feature_importances_[x]) for x in range(len(rf.feature_importances_))]\n",
    "    feats = np.array(sorted(arr, key=lambda x: x[1], reverse=True))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd9feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = sk.fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fca9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/vmrx_nc16tz65xmfmz0x56kh0000gn/T/ipykernel_6430/1392914790.py:6: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  plt.xticks([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feats = run_forest_run(X, y)\n",
    "names = list(feats[:, 0])\n",
    "names.reverse()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(names, feats[:, 1], color = 'green')\n",
    "plt.xticks([])\n",
    "plt.title('Feature Importance', size=13)\n",
    "plt.xlabel('Value of Importance', size=13)\n",
    "plt.ylabel('Feature Names', size=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68aa694",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "As we can see here, most of our values did better than random noise but that's not always the case. In the event our data is absolutely atrocious, noise has a possibility to be our highest predictor. In reality that usually isn't the case but it theorhetically could happen. \n",
    "\n",
    "What about the values that do slightly better than noise? We can concretely say values that did worse than randomly picking are not good at predicting, but where do we draw the line between bad and good? Well we can just try a different feature importance method and see if it lines up.\n",
    "\n",
    "## 3 Feature Permutation\n",
    "This is my next favorite feature importance method because it's really easy to implement and understand. We run a regression with each column and our target variable, and then do it again with our feature indices shuffled. The theory behind it is that if the feature is important, shuffling around its data will vastly ruin the results. Vice versa, if a feature is not very important, shuffling its data around may not change the result much. It's important to use a good metric or model here that captures the relationship between your features and your target variable. For example, using mean absolute error with values on very different scales may lead to some incorrect conclusions. To avoid this I simply standardized both sides. Here is a sample implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28a5341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Ranked by Drop in MSE:\n",
      "\n",
      "Feature: MedInc\n",
      "Original MSE: 0.623849584082904\n",
      "Shuffled MSE: 1.9978189555233081\n",
      "\n",
      "Feature: AveRooms\n",
      "Original MSE: 1.6961034205170844\n",
      "Shuffled MSE: 1.9776063713524719\n",
      "\n",
      "Feature: HouseAge\n",
      "Original MSE: 1.7887531750135799\n",
      "Shuffled MSE: 1.9957984013922356\n",
      "\n",
      "Feature: AveOccup\n",
      "Original MSE: 2.0474748259122686\n",
      "Shuffled MSE: 2.011435022622182\n",
      "\n",
      "Feature: Population\n",
      "Original MSE: 2.0492993577777896\n",
      "Shuffled MSE: 2.0106582610717116\n",
      "\n",
      "Feature: Longitude\n",
      "Original MSE: 2.0919332302359566\n",
      "Shuffled MSE: 2.01641903994245\n",
      "\n",
      "Feature: AveBedrms\n",
      "Original MSE: 2.093401025938974\n",
      "Shuffled MSE: 1.9895411428871816\n",
      "\n",
      "Feature: Latitude\n",
      "Original MSE: 2.2883205537493185\n",
      "Shuffled MSE: 1.982975541345169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feats = []\n",
    "y_std = stats.zscore(y)\n",
    "for c in X.columns:\n",
    "    X_std = stats.zscore(X[c])\n",
    "    mse = mean_squared_error(X_std, y_std)\n",
    "    mse_shuffled = mean_squared_error(X_std.sample(frac=1, random_state=171), y_std)\n",
    "    feats.append([c, mse, mse_shuffled, mse_shuffled-mse])\n",
    "print('Features Ranked by Drop in MSE:\\n')\n",
    "for c in sorted(feats, key=(lambda x: x[3]), reverse=True):\n",
    "    print(f'Feature: {c[0]}')\n",
    "    print(f'Original MSE: {c[1]}')\n",
    "    print(f'Shuffled MSE: {c[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042a9e6",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "According to this list, we have some features that are not as important as we thought and now we have some features that did better when we shuffled around their indices. This would imply they're not very important or we just got unlucky and the indices created a fictional correlation. Aside from MedInc, its not clear how much (if any) significance the other features have in predicting our target or more importantly, we're not sure which features we should use as a final model to predit our target variable. \n",
    "\n",
    "## 4 Linear Regression p-values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290608c",
   "metadata": {},
   "source": [
    "Another method of feature importance is very simply, looking at the p-values when doing a linear regression. The way I like to think about p-values is the probability that the corresponding variable is not significant. This is because it's calculated using a t-test which compares the sum of squared residuals when the coefficient of the feature is zero versus it being non-zero. The typical p-value border is 0.05. If something has a p-value below 0.05, it's significant and if its above its not. Here's an example with our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9217e201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3970.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 09 Mar 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:20:44</td>     <th>  Log-Likelihood:    </th> <td> -22624.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 20640</td>      <th>  AIC:               </th> <td>4.527e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 20631</td>      <th>  BIC:               </th> <td>4.534e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>  -36.9419</td> <td>    0.659</td> <td>  -56.067</td> <td> 0.000</td> <td>  -38.233</td> <td>  -35.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MedInc</th>     <td>    0.4367</td> <td>    0.004</td> <td>  104.054</td> <td> 0.000</td> <td>    0.428</td> <td>    0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HouseAge</th>   <td>    0.0094</td> <td>    0.000</td> <td>   21.143</td> <td> 0.000</td> <td>    0.009</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveRooms</th>   <td>   -0.1073</td> <td>    0.006</td> <td>  -18.235</td> <td> 0.000</td> <td>   -0.119</td> <td>   -0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveBedrms</th>  <td>    0.6451</td> <td>    0.028</td> <td>   22.928</td> <td> 0.000</td> <td>    0.590</td> <td>    0.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Population</th> <td>-3.976e-06</td> <td> 4.75e-06</td> <td>   -0.837</td> <td> 0.402</td> <td>-1.33e-05</td> <td> 5.33e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AveOccup</th>   <td>   -0.0038</td> <td>    0.000</td> <td>   -7.769</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Latitude</th>   <td>   -0.4213</td> <td>    0.007</td> <td>  -58.541</td> <td> 0.000</td> <td>   -0.435</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Longitude</th>  <td>   -0.4345</td> <td>    0.008</td> <td>  -57.682</td> <td> 0.000</td> <td>   -0.449</td> <td>   -0.420</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4393.650</td> <th>  Durbin-Watson:     </th> <td>   0.885</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>14087.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.082</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 6.420</td>  <th>  Cond. No.          </th> <td>2.38e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.38e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.606\n",
       "Model:                            OLS   Adj. R-squared:                  0.606\n",
       "Method:                 Least Squares   F-statistic:                     3970.\n",
       "Date:                Thu, 09 Mar 2023   Prob (F-statistic):               0.00\n",
       "Time:                        18:20:44   Log-Likelihood:                -22624.\n",
       "No. Observations:               20640   AIC:                         4.527e+04\n",
       "Df Residuals:                   20631   BIC:                         4.534e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -36.9419      0.659    -56.067      0.000     -38.233     -35.650\n",
       "MedInc         0.4367      0.004    104.054      0.000       0.428       0.445\n",
       "HouseAge       0.0094      0.000     21.143      0.000       0.009       0.010\n",
       "AveRooms      -0.1073      0.006    -18.235      0.000      -0.119      -0.096\n",
       "AveBedrms      0.6451      0.028     22.928      0.000       0.590       0.700\n",
       "Population -3.976e-06   4.75e-06     -0.837      0.402   -1.33e-05    5.33e-06\n",
       "AveOccup      -0.0038      0.000     -7.769      0.000      -0.005      -0.003\n",
       "Latitude      -0.4213      0.007    -58.541      0.000      -0.435      -0.407\n",
       "Longitude     -0.4345      0.008    -57.682      0.000      -0.449      -0.420\n",
       "==============================================================================\n",
       "Omnibus:                     4393.650   Durbin-Watson:                   0.885\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14087.596\n",
       "Skew:                           1.082   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.420   Cond. No.                     2.38e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.38e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = ''\n",
    "for c in X.columns:\n",
    "    string += c + '+'\n",
    "string = string[:-1]\n",
    "Xy = X.copy()\n",
    "Xy['target'] = y\n",
    "lr = smf.ols(f'target ~ {string}', data=Xy).fit()\n",
    "Xy.drop('target', axis=1, inplace=True)\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595d8c0",
   "metadata": {},
   "source": [
    "In our linear regression model summary, it tells us that in fact all of our variables are significant except for Population. This adds up with our initial feature selection methods because Population is always at the bottom and MedInc is always at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc937a",
   "metadata": {},
   "source": [
    "## 5 Summary\n",
    "Although there definitely is more ways to find important features, these are three quick and easy methods to find which features significantly predict unseen data accurately. The reason there might be some variation from method to method or why some features may not seem as important as you think they should be using domain knowledge for instance, is because the data might be missing important features or two or more features are highly correlated which might bring both of their individual importances down even though they may both be important. Fixing this would be more of a feature engineering issue than feature importance which is why I did not cover it in this notebook. Overall, it's impossible to know the ground truth of feature combinations that actually make up our data so using different methods of feature importance is important to learn more about our data and use that information to make the best model that will predict unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
